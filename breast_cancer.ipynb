# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import GridSearchCV

# Load the dataset
url = "https://raw.githubusercontent.com/yourusername/dataset.csv"  # Update this with your dataset's URL or local path
df = pd.read_csv(url)

# Check the first few rows of the dataset
df.head()

# Data Preprocessing
# Handling missing values (if any)
df.isnull().sum()  # Checking for missing values

# Example: Fill missing values with mean or drop rows/columns
df.fillna(df.mean(), inplace=True)

# Drop any irrelevant columns (if needed)
df.drop(columns=['Unnamed: 32', 'id'], axis=1, inplace=True)

# Feature Engineering (encoding categorical variables if any)
# In this case, all features are numeric, so no need for encoding.

# Define the features (X) and target (y)
X = df.drop(columns=['diagnosis'])  # All columns except 'diagnosis'
y = df['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)  # Convert 'M' to 1 (Malignant) and 'B' to 0 (Benign)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model Training - Logistic Regression
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred_log_reg = log_reg.predict(X_test_scaled)

# Model Evaluation - Logistic Regression
print("Logistic Regression Classification Report:")
print(classification_report(y_test, y_pred_log_reg))

# Model Training - Random Forest Classifier
rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred_rf = rf_clf.predict(X_test_scaled)

# Model Evaluation - Random Forest Classifier
print("Random Forest Classifier Classification Report:")
print(classification_report(y_test, y_pred_rf))

# Model Training - Support Vector Machine (SVM)
svm_clf = SVC(random_state=42)
svm_clf.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred_svm = svm_clf.predict(X_test_scaled)

# Model Evaluation - Support Vector Machine
print("SVM Classifier Classification Report:")
print(classification_report(y_test, y_pred_svm))

# Model Performance Comparison
models = ['Logistic Regression', 'Random Forest', 'SVM']
accuracies = [accuracy_score(y_test, y_pred_log_reg), accuracy_score(y_test, y_pred_rf), accuracy_score(y_test, y_pred_svm)]

plt.figure(figsize=(8, 6))
sns.barplot(x=models, y=accuracies)
plt.title('Model Comparison: Accuracy')
plt.ylabel('Accuracy')
plt.show()

# Hyperparameter Tuning - Example for Random Forest using GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search_rf = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search_rf.fit(X_train_scaled, y_train)

# Best parameters from GridSearchCV
print("Best parameters for Random Forest:", grid_search_rf.best_params_)

# Evaluate the tuned model
best_rf = grid_search_rf.best_estimator_
y_pred_best_rf = best_rf.predict(X_test_scaled)

# Final evaluation of the tuned model
print("Tuned Random Forest Classification Report:")
print(classification_report(y_test, y_pred_best_rf))

# Save the model (optional)
import joblib
joblib.dump(best_rf, 'breast_cancer_prediction_model.pkl')

